import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image

import torchvision
import torchvision.transforms as transforms

# from matplotlib import pyplot as plt
# import seaborn as sn
import pandas as pd
import numpy as np
import os
import torchvision.models as models

device = 'cuda'
------------------------------------------------------------
import torchvision
import torchvision.transforms as transforms
import os
from torch.utils.data import random_split

# Define the root path to the train and test folders
root_folder = 'C:/Users/RndDev/Desktop/Koln Model/Dataset1'

# Define subfolder paths
train_folder = os.path.join(root_folder, 'train')
test_folder = os.path.join(root_folder, 'test')

transform_train = transforms.Compose([

    transforms.RandomResizedCrop(size=(256, 256), scale=(0.5, 1.0), ratio=(3 / 4, 4 / 3)),
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5]
    )
])

transform_val = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5]
    )
])

transform_test = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5]
    )
])

# Create custom datasets using the ImageFolder dataset class
train_dataset = torchvision.datasets.ImageFolder(root=train_folder, transform=transform_train)
test_dataset = torchvision.datasets.ImageFolder(root=test_folder, transform=transform_test)


# Set batch sizes and create data loaders
batch_size_train = 10
batch_size_val = 10
batch_size_test = 10
------------------------------------------------------------
# plt.rcParams['figure.figsize'] = [30, 8]
# plt.rcParams['figure.dpi'] = 60
# plt.rcParams.update({'font.size': 20})


# def imshow(input):
#     # torch.Tensor => numpy
#     input = input.numpy().transpose((1, 2, 0))
#     # undo image normalization
#     mean = np.array([0.5, 0.5, 0.5])
#     std = np.array([0.5, 0.5, 0.5])
#     input = std * input + mean
#     input = np.clip(input, 0, 1)
#     # display images
#     plt.imshow(input)
#     plt.show()


class_names = {
  0: "IU",
  1: "kimyouna",
  2: "ive_JWY",
  3: "bts_jimin",
  4: "rv_irene",
  5: "aespa_karina",
  6: "stkiz_hyunjin",
  7: "bp_jisoo",
  8: "ive_ahnyoojin",
  9: "shinee_minho",
  10: "idle_yuqi",
  11: "shinee_key",
  12: "shinee_taemin",
  13: "btob_yooksungjae",
  14: "kangdongwon",
  15: "idle_shuhwa",
  16: "parkhaejin",
  17: "ahnbohyun"
}
--------------------------------------------------------
learning_rate = 0.002
log_step = 20
n_classes = 18

# model = models.densenet121(pretrained=True)
# num_features = model.classifier.in_features
# model.classifier = nn.Linear(num_features, n_classes) # transfer learning
# model = model.cuda()

criterion = nn.CrossEntropyLoss()
# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
--------------------------------------------------------------------
import time


def train(train_loader, model, optimizer):
    start_time = time.time()
    # print(epoch,"ffff")
    print(f'[Epoch: {epoch + 1} - Training]')
    model.train()
    total = 0
    running_loss = 0.0
    running_corrects = 0

    for i, batch in enumerate(train_loader):
        
        imgs, labels = batch
        # imgs, labels = imgs.cuda(), labels.cuda()

        outputs = model(imgs)
        optimizer.zero_grad()
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        total += labels.shape[0]
        running_loss += loss.item()
        running_corrects += torch.sum(preds == labels.data)

        if i % log_step == log_step - 1:
            print(f'[Batch: {i + 1}] running train loss: {running_loss / total}, running train accuracy: {running_corrects / total}')

    print(f'train loss: {running_loss / total}, accuracy: {running_corrects / total}')
    print("elapsed time:", time.time() - start_time)
    return running_loss / total, (running_corrects / total).item()


def validate(val_loader, model):
    start_time = time.time()
    print(f'[Epoch: {epoch + 1} - Validation]')
    model.eval()
    total = 0
    running_loss = 0.0
    running_corrects = 0

    for i, batch in enumerate(val_loader):
        imgs, labels = batch
        # imgs, labels = imgs.cuda(), labels.cuda()

        with torch.no_grad():
            outputs = model(imgs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

        total += labels.shape[0]
        running_loss += loss.item()
        running_corrects += torch.sum(preds == labels.data)

        if (i == 0) or (i % log_step == log_step - 1):
            print(f'[Batch: {i + 1}] running val loss: {running_loss / total}, running val accuracy: {running_corrects / total}')

    print(f'val loss: {running_loss / total}, accuracy: {running_corrects / total}')
    print("elapsed time:", time.time() - start_time)
    return running_loss / total, (running_corrects / total).item()


def test(test_loader, model):
    start_time = time.time()
    print(f'[Test]')
    model.eval()
    total = 0
    running_loss = 0.0
    running_corrects = 0

    for i, batch in enumerate(test_loader):
        print("Iteration: ", i)
        imgs, labels = batch
        # imgs, labels = imgs.cuda(), labels.cuda()

        with torch.no_grad():
            outputs = model(imgs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

        total += labels.shape[0]
        running_loss += loss.item()
        running_corrects += torch.sum(preds == labels.data)

        if (i == 0) or (i % log_step == log_step - 1):
            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}')

    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}')
    print("elapsed time:", time.time() - start_time)
    return running_loss / total, (running_corrects / total).item()
-----------------------------------------------------------------------------------------------
def adjust_learning_rate(optimizer, epoch):
    lr = learning_rate
    if epoch >= 5:
        lr /= 10
    if epoch >= 10:
        lr /= 10
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
 -------------------------------------------------------------------------       
#train / val 9:1
dataset_size = len(train_dataset
train_size = int(dataset_size * 0.9)
val_size = dataset_size - train_size

def create_bootstrap_sample(dataset, n_samples):
    bootstrapped_datasets = []
    for _ in range(n_samples):
        bootstrapped_datasets.append(random_split(dataset, [train_size, val_size]))
    return bootstrapped_datasets
------------------------------------------------------------------------------    
  num_bagging_models = 2

bootstrap_samples = create_bootstrap_sample(train_dataset, num_bagging_models)


for i in range(num_bagging_models):
    print(f'Training Model {i+1}/{num_bagging_models}')
    model = models.densenet121(pretrained=True)
    num_features = model.classifier.in_features
    print('In features: ', num_features)
    model.classifier = nn.Linear(num_features, n_classes)
    # model.to(device)  # Move model to GPU if available
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    
    train_loader = torch.utils.data.DataLoader(bootstrap_samples[i][0], batch_size=batch_size_train, shuffle=True, num_workers=4)
    val_loader = torch.utils.data.DataLoader(bootstrap_samples[i][1], batch_size=batch_size_val, shuffle=False, num_workers=4)
    
    num_epochs = 25
    best_val_acc = 0
    best_epoch = 0

    history = []
    accuracy = []
    for epoch in range(num_epochs):
        adjust_learning_rate(optimizer, epoch)
        print(epoch,"fgggh")
        try:
            train_loss, train_acc = train(train_loader, model, optimizer)
            val_loss, val_acc = validate(val_loader, model)
            history.append((train_loss, val_loss))
            accuracy.append((train_acc, val_acc))
        except Exception as e:
            print(str(e),"ns")    

        if val_acc > best_val_acc:
            print("[Info] best validation accuracy!")
            best_val_acc = val_acc
            best_epoch = epoch
            torch.save(model.state_dict(), f'best_model{i}_checkpoint_epoch_{epoch + 1}.pth')

    torch.save(model.state_dict(), f'DenseNet_bagging_best_epoch_{i}.pth')
    
    
